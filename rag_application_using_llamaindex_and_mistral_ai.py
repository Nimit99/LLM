# -*- coding: utf-8 -*-
"""RAG_Application_Using_LlamaIndex_and_Mistral_AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14y5G_Y_d1oV9zpKw9-MVtcFmv2vAzfs-
"""

from google.colab import drive

# This will prompt for authorization.
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %pip install llama-index-llms-huggingface

!pip install llama-index

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.huggingface import HuggingFaceLLM

# !mkdir data

# load documents
documents = SimpleDirectoryReader("./data/").load_data()

print(documents)

# setup prompts - specific to StableLM
from llama_index.core import PromptTemplate

system_prompt = """<|SYSTEM|># You are a Q&A assistant. Your goal is to answer questions as
accurately as possible based on the instructions and context provided.
"""

# This will wrap the default prompts that are internal to llama-index
query_wrapper_prompt = PromptTemplate("<|USER|>{query_str}<|ASSISTANT|>")

"""https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/llms/llama-index-llms-huggingface/llama_index/llms/huggingface/base.py"""



import os

# Set your Hugging Face token
os.environ["HF_TOKEN"] = "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"

import torch

llm = HuggingFaceLLM(
    context_window=4096,
    max_new_tokens=256,
    generate_kwargs={"temperature": 0.7, "do_sample": False},
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name="mistralai/Mistral-7B-Instruct-v0.1",
    model_name="mistralai/Mistral-7B-Instruct-v0.1",
    device_map="auto",
    stopping_ids=[50278, 50279, 50277, 1, 0],
    tokenizer_kwargs={"max_length": 4096},
    # uncomment this if using CUDA to reduce memory usage
    model_kwargs={"torch_dtype": torch.float16}
)

# Commented out IPython magic to ensure Python compatibility.
# %pip install llama-index-embeddings-huggingface
# %pip install llama-index-embeddings-instructor



from llama_index.embeddings.huggingface import HuggingFaceEmbedding
embed_model =HuggingFaceEmbedding(model_name="sentence-transformers/all-mpnet-base-v2")

"""https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/service_context.py"""

from llama_index.core import VectorStoreIndex, ServiceContext

service_context = ServiceContext.from_defaults(
    chunk_size=1024,
    llm=llm,
    embed_model=embed_model
)

index = VectorStoreIndex.from_documents(documents, service_context=service_context)

query_engine = index.as_query_engine()

import torch

# Check the current GPU memory usage (optional, for information purposes)
def print_gpu_memory():
    t = torch.cuda.get_device_properties(0).total_memory
    r = torch.cuda.memory_reserved(0)
    a = torch.cuda.memory_allocated(0)
    f = r-a  # free inside reserved
    print(f"Total GPU Memory: {t} bytes\nReserved Memory: {r} bytes\nAllocated Memory: {a} bytes\nFree Memory: {f} bytes")

print_gpu_memory()

# Clear unused memory
torch.cuda.empty_cache()
print("Cleared unused memory.")

# Assuming `query_engine` is already set up as shown in your previous messages
query = "what treatment should we give for Painless, right-sided abdominal mass?"
response = query_engine.query(query)
print("Query Response:", response)

# Print GPU memory after running the query
print_gpu_memory()

import torch

# Check the current GPU memory usage (optional, for information purposes)
def print_gpu_memory():
    t = torch.cuda.get_device_properties(0).total_memory
    r = torch.cuda.memory_reserved(0)
    a = torch.cuda.memory_allocated(0)
    f = r-a  # free inside reserved
    print(f"Total GPU Memory: {t} bytes\nReserved Memory: {r} bytes\nAllocated Memory: {a} bytes\nFree Memory: {f} bytes")

print_gpu_memory()

# Clear unused memory
torch.cuda.empty_cache()
print("Cleared unused memory.")

query = "how many collumn treatment is?"
response = query_engine.query(query)
print("Query Response:", response)



import torch

def query_model(query_engine):
    # Function to manage GPU memory without showing the details to the user
    def manage_gpu_memory():
        torch.cuda.empty_cache()  # Clear unused memory quietly

    # Prompt the user for a query
    user_query = input("Please enter your query: ")

    # Manage GPU memory before processing the query
    manage_gpu_memory()

    # Process the query
    response = query_engine.query(user_query)

    # Output the response
    print("\nResponse:")
    print(response)

    # Optionally manage GPU memory after processing if needed
    manage_gpu_memory()

# Assuming `query_engine` is already set up as shown in your previous messages
query_model(query_engine)

import torch

def query_model(query_engine):
    # Function to manage GPU memory without showing the details to the user
    def manage_gpu_memory():
        torch.cuda.empty_cache()  # Clear unused memory quietly

    # Prompt the user for a query
    user_query = input("Please enter your query: ")

    # Manage GPU memory before processing the query
    manage_gpu_memory()

    # Process the query
    response = query_engine.query(user_query)

    # Output the response
    print("\nResponse:")
    print(response)

    # Optionally manage GPU memory after processing if needed
    manage_gpu_memory()

# Assuming `query_engine` is already set up as shown in your previous messages
query_model(query_engine)









